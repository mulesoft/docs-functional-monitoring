= Upload a Monitor
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

You can upload a suite of tests that you have written and want to run as a monitor, viewing the results the information displays in the *Functional Monitoring* section of Anypoint Monitoring. When you create monitors by writing them, you can create more complex test cases by using features such as the DataWeave expression language and context variables for saving values between calls to endpoints.


== Before You Begin

* Ensure that your user ID has the Monitoring Center User permission. An administrator for your organization can grant you this permission in Anypoint Access Management.

* Write a suite of tests in the BDD test-writing language. For information about writing tests, see "API Functional Monitoring with the BAT CLI", which is linked to from the *See also* section at the end of this topic.

* Determine the frequency at which you would like the monitor to endpoints in the test suite. The shortest interval is every fifteen minutes. You can also choose to run the monitor hourly, daily, weekly, or monthly. The schedule begins immediately after you upload the monitor. After you upload the monitor, you can create another schedule that is more detailed, if you prefer.

* Determine which execution location you would like to run the monitor in.

* If you have more than one configuration file in your test suite, determine which you plan to use for the monitor.

== Procedure
To create a monitor by uploading a test suite:

. In the *Functional Monitoring* section of Anypoint Monitoring, click *Upload Monitor*.
. Upload the folder that contains your test suite.
. Fill in the fields with the information that you have gathered.
. Click *Upload Monitor*.

== Result

The monitor detail opens. An indicator displays how long the execution ran. The last executions list displays the test result.

== What to do next

You can view a list of monitor schedules in the schedules table. You can also disable and enable schedules.

The last executions table and the average times chart will enable you to determine the following:

* Whether the endpoint returned the expected code, as indicated by *PASSED* or *FAILED*
* Statistics about the execution of the endpoint test

As your monitor continues to run at the scheduled intervals, the statistics in this table are updated, as well as the graph.

== See also

* xref:bat-top.adoc[API Functional Monitoring with the BAT CLI]

= Uploading Monitors

You can upload a suite of tests that you have written and want to run as a monitor, viewing the results the information displays in the *Functional Monitoring* section of Anypoint Monitoring. When you create monitors by writing them, you can create more complex test cases by using features such as the DataWeave expression language and context variables for saving values between calls to endpoints.


== Before You Begin

* Ensure that your user ID has the Monitoring Center User permission. An administrator for your organization can grant you this permission in Anypoint Access Management.

* Write a suite of tests in the BDD test-writing language. For information about writing tests, see xref:bat-top.adoc[API Functional Monitoring with BAT CLI].

* Determine the frequency at which you would like the monitor to endpoints in the test suite. The shortest interval is every fifteen minutes. You can also choose to run the monitor hourly, daily, weekly, or monthly. The schedule begins immediately after you upload the monitor. After you upload the monitor, you can create another schedule that is more detailed, if you prefer.

* Determine which execution location you would like to run the monitor in.

* If you have more than one configuration file in your test suite, determine which you plan to use for the monitor.

== Upload a Monitor
To create a monitor by uploading a test suite:

. In the *Functional Monitoring* section of Anypoint Monitoring, click *Upload Monitor*.
. Select the folder that contains your test suite.
+
For details on file structure and file requirements, see xref:bat-example-test-suite.adoc[Example of a Test Suite].

. Click *Upload Monitor*.
. Click *Next*.
+
The *Monitor Detail* page opens. 
+
. In the *Last executions* list, you can view the test result and how long the test ran.

== What To Do Next

You can view a list of monitor schedules in the schedules table. You can also disable and enable schedules.

The last executions table and the average times chart enable you to determine:

* Whether the endpoint returned the expected code, as indicated by *PASSED* or *FAILED*
* Statistics about the execution of the endpoint test

As your monitor continues to run at the scheduled intervals, the statistics in this table are updated, as well as the graph.

== See Also

* xref:bat-top.adoc[API Functional Monitoring with BAT CLI]
